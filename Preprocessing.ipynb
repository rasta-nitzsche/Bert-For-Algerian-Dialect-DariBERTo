{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.metrics import Accuracy,Recall,Precision\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rasta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "LOW = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "LATIN = \"Ààéêèç\"\n",
    "SPACE = '\\t\\n\\r\\v\\f'\n",
    "fr_stopwords=stopwords.words('french')\n",
    "\n",
    "LETTERS = 'ابتةثجحخدذرزسشصضطظعغفقكلمنهويءآأؤإؤ'\n",
    "\n",
    "#all_latin_chars = UP + LOW + LATIN\n",
    "\n",
    "punctuations = string.punctuation\n",
    "\n",
    "punctuations_list =  punctuations + SPACE\n",
    "\n",
    "french_punctuations_list= punctuations_list.replace(\"'\",\"\")\n",
    "\n",
    "#Full List of emogies to be removed\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                       #u\"\\U0001F600-\\U0001F64F\"  # emoticons to keep \n",
    "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                       u\"\\U00002702-\\U000027B0\"\n",
    "                       u\"\\U000024C2-\\U0001F251\"\n",
    "                       u\"\\U0001f932\"\n",
    "                       u\"\\u200f\"\n",
    "                       u\"\\U0001F914\"\n",
    "                       u\"\\U0001F923\"\n",
    "                       u\"\\u200D\"\n",
    "                       u\"\\u202c\"\n",
    "                       u\"\\u2069\"\n",
    "                       u\"\\u2066\"\n",
    "                       u\"\\U0001F926\"\n",
    "                       u\"\\U0001F917\"\n",
    "                       u\"\\U0001f928\"\n",
    "                       u\"\\t\"\n",
    "                       u\"\\u200e\"\n",
    "                       \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "print(punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove small words \n",
    "def remove_minlen_word(text, threshold):\n",
    "    return \" \".join([w for w in text.split() if len(w) > threshold])\n",
    "\n",
    "#lower text\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "#Remove urls\n",
    "def remove_url(text):\n",
    "    text = re.sub('http[s]?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('ftp?://\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub('www.\\S+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "\n",
    "#remove tags @ and hashtags #\n",
    "def remove_prefix(text, prefix):  # mainly used to remove hashtags ans mensions\n",
    "    purged=[]\n",
    "    output=\"\"\n",
    "    for word in text.split():\n",
    "        if not word.startswith(prefix):\n",
    "            purged.append(word)\n",
    "    return \" \".join(purged)\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_french_punctuations(text):\n",
    "    translator = str.maketrans('', '', french_punctuations_list)\n",
    "    return text.translate(translator)\n",
    "\n",
    "#remove french stop words, here we specified just Fr stop words, you can add other DZ word that you juge as stop words \n",
    "def remove_fr_stop_words(text):\n",
    "    temp_text=[]\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in fr_stopwords:\n",
    "            temp_text.append(word)\n",
    "    return ' '.join(temp_text)\n",
    "\n",
    "# Remove repetting letters \n",
    "def remove_repeating_char(text):\n",
    "    result=[]\n",
    "    for word in text.split(\" \"):\n",
    "        temp=re.sub(r'(.)\\1+', r'\\1', word)\n",
    "        if len(temp)==1:\n",
    "             temp=temp+temp\n",
    "        result.append( temp)\n",
    "    return \" \".join(result)\n",
    "\n",
    "# remove emogies \n",
    "def remove_emoji(text):\n",
    "    return emoji_pattern.sub(r' ' , text)\n",
    "\n",
    "# remove extra stop words \n",
    "def remove_extra_white_spaces(text):\n",
    "    text= ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# remove digits delimited with white space\n",
    "def remove_digits(text):\n",
    "    text= re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# specify a list of generic words to be removed if you want =W we can add here the arabizi stop words \n",
    "def remove_words_generic(text,list_words=[]):\n",
    "    purged=[]\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in list_words:\n",
    "            purged.append(word)\n",
    "    return \" \".join(purged)\n",
    "\n",
    "\n",
    "#remove all arabic characters from a text\n",
    "def remove_arabic_chars(text):\n",
    "    new_text = \"\"\n",
    "    for char in text:\n",
    "        if char not in LETTERS:\n",
    "            new_text += char\n",
    "    return new_text\n",
    "\n",
    "#normalize arabic numbers\n",
    "def normalize_arabic_numbers(text):\n",
    "    text = re.sub(\"٠\", \"0\", text)\n",
    "    text = re.sub(\"١\", \"1\", text)\n",
    "    text = re.sub(\"٢\", \"2\", text)\n",
    "    text = re.sub(\"٣\", \"3\", text)\n",
    "    text = re.sub(\"٤\", \"4\", text)\n",
    "    text = re.sub(\"٥\", \"5\", text)\n",
    "    text = re.sub(\"٦\", \"6\", text)\n",
    "    text = re.sub(\"٧\", \"7\", text)\n",
    "    text = re.sub(\"٨\", \"8\", text)\n",
    "    text = re.sub(\"٩\", \"9\", text)\n",
    "    return text\n",
    "\n",
    "# normalize arabizi characters\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"2\", \"a\", text)\n",
    "    text = re.sub(\"3\", \"aa\", text)\n",
    "    text = re.sub(\"7\", \"h\", text)\n",
    "    text = re.sub(\"9\", \"q\", text)\n",
    "    text = re.sub(\"8\",\"gh\", text)\n",
    "    text = re.sub(\"5\", \"kh\", text)\n",
    "    text = re.sub(\"sh\",\"ch\", text)\n",
    "    text = re.sub(\"dj\",\"j\", text)\n",
    "    return text\n",
    "###################################################################################################\n",
    "#Data Cleaning and Lemmatization\n",
    "def remove_noise(text):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('french')\n",
    "\n",
    "    for token, tag in pos_tag(nltk.word_tokenize(text)):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "#remove vowel\n",
    "def rem_vowel(text):\n",
    "        vowels = ('a', 'e', 'i', 'o', 'u','é','à','è')\n",
    "        for x in text.lower():\n",
    "            if x in vowels:\n",
    "                text = text.replace(x, \"\")\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the pipeline of preprocessing \n",
    "def my_preproc(data=\"\",list_words=[]):\n",
    "    text = remove_url(data)\n",
    "    text= lower(text)\n",
    "    text= remove_minlen_word(text,1)  #words containing one letter\n",
    "    text=normalize_arabic_numbers(text) \n",
    "    text=remove_arabic_chars(text)\n",
    "    text=remove_digits(text) \n",
    "    text=normalize_arabic(text)\n",
    "    text=remove_prefix(text,\"@\")\n",
    "    text=remove_prefix(text,\"#\")\n",
    "    text=remove_words_generic(text, list_words) \n",
    "    text=remove_french_punctuations(text) \n",
    "    text=remove_fr_stop_words(text) \n",
    "    text=remove_repeating_char(text)\n",
    "    text=remove_extra_white_spaces(text) \n",
    "    text=remove_emoji(text)\n",
    "   # text=rem_vowel(text)\n",
    "   # text=remove_noise(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_t(text):\n",
    "    if text.endswith('yt'):\n",
    "        text = text[:-1] + \"a\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_preproc_arabic(data=\"\",list_words=[]):\n",
    "    text = remove_url(data)\n",
    "    text= lower(text)\n",
    "    text= remove_minlen_word(text,1)  #words containing one letter\n",
    "    text=normalize_arabic_numbers(text) \n",
    "    text=remove_arabic_chars(text)\n",
    "    text=remove_digits(text) \n",
    "    text=normalize_arabic(text)\n",
    "    text=remove_prefix(text,\"@\")\n",
    "    text=remove_prefix(text,\"#\")\n",
    "    text=remove_words_generic(text, list_words) \n",
    "    text=remove_french_punctuations(text) \n",
    "    text=remove_fr_stop_words(text) \n",
    "    text=remove_repeating_char(text)\n",
    "    text=remove_extra_white_spaces(text) \n",
    "    text=remove_emoji(text)\n",
    "    text=remove_t(text)\n",
    "   # text=rem_vowel(text)\n",
    "   # text=remove_noise(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rani haba nroh alghia'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"rani 7aba nro7 l alg8ia 187\"\n",
    "my_preproc(data=text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(comments):\n",
    "    df = pd.Series(comments)\n",
    "    df.to_csv('out.csv', index=False, header=False)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('dataset.csv',header=None)\n",
    "df=data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(lambda x:my_preproc(data=str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "\n",
    "PRETRAINED_MODEL_PATH = 'lid.176.bin'\n",
    "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
    "\n",
    "def predict_language(text):\n",
    "    predictions = model.predict([text])\n",
    "    return str(predictions[0])[12:14], predictions[1][0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('arabic_dataset.csv',header=None)\n",
    "df=data[0]\n",
    "df = df.apply(lambda x:my_preproc_arabic(data=str(x)))\n",
    "df = df.drop_duplicates()\n",
    "write_to_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('preprocessed_datased.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alah ibarek artistes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yo voudrais jouer dayz xbox gamertag vaxfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salam mabrok awachrkom daiw maya chi doriya sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>merci beaucoup monsieur jamal l'efort l'explic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17340</th>\n",
       "      <td>abdelhak zwin mardi lwalidin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17341</th>\n",
       "      <td>top khouya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17342</th>\n",
       "      <td>ca rapel bele epoque rai q0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17343</th>\n",
       "      <td>c'est trop bien</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17344</th>\n",
       "      <td>ok cete playlist done envie rester superconfite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17345 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0\n",
       "0                                   alah ibarek artistes\n",
       "1           yo voudrais jouer dayz xbox gamertag vaxfire\n",
       "2      salam mabrok awachrkom daiw maya chi doriya sa...\n",
       "3      merci beaucoup monsieur jamal l'efort l'explic...\n",
       "4                                                   amen\n",
       "...                                                  ...\n",
       "17340                       abdelhak zwin mardi lwalidin\n",
       "17341                                         top khouya\n",
       "17342                        ca rapel bele epoque rai q0\n",
       "17343                                    c'est trop bien\n",
       "17344    ok cete playlist done envie rester superconfite\n",
       "\n",
       "[17345 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
